{
    "docs": [
        {
            "location": "/",
            "text": "Setup [15 minutes]\n\n\nThis tutorial is designed to be run on Amazon EC2 using the\nUbuntu Server 14.04 LTS image, although it should be trivial to port\nit for use on any other UNIX operating system.\nI've tested that everything works on an m3.medium instance.\nIf you would like to use Windows, Git-Bash (packaged with Git for Windows)\nis probably your best bet, although it has not been tested on that platform.\n\n\nFor this lesson we will be using an already prepared set of files.\n\n\ncurl https://codeload.github.com/bsmith89/make-example/tar.gz/master \\\n    > make-example-master.tgz\ntar -xzf make-example-master.tgz\ncd make-example-master\n\n\n\n\nLet's take a look at the files we will be working with:\n\n\nsudo apt-get update\nsudo apt-get install tree\ntree\n\n\n\n\nThe \ntree\n command produces a handy tree-diagram of the directory.\n\n\n.\n\u251c\u2500\u2500 books\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 abyss.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isles.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 last.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE_TEXTS.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sierra.txt\n\u251c\u2500\u2500 LICENSE.md\n\u251c\u2500\u2500 matplotlibrc\n\u251c\u2500\u2500 plotcount.py\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 wordcount.py\n\n1 directory, 7 files\n\n\n\n\nBe sure that you also have \nPython 3\n, \nGit\n, and \nGNU Make\n.\n\n\nsudo apt-get install git make\n\n\n\n\nConfigure git.\n\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email you@example.com\n\n\n\n\nInstall matplotlib.\n\n\nsudo apt-get install python3-matplotlib\n\n\n\n\nAnd, finally, load up your favorite terminal multi-plexer so we can recover if\nwe get disconnected.\n\n\ntmux\n\n\n\n\nMotivation [30 minutes]\n\n\n\n\nThe most frequently-occurring word occurs approximately twice as\noften as the second most frequent word. This is\n\nZipf's Law\n.\n\n\n\n\nLet's imagine that instead of computational biology we're interested in\ntesting Zipf's law in some of our favorite books.\nWe've compiled our raw data, the books we want to analyze\n(check out \nhead books/isles.txt\n)\nand have prepared several Python scripts that together make up our\nanalysis pipeline.\n\n\nBefore we begin, add a README to your project describing what we intend\nto do.\n\n\nnano README.md\n# Describe what you're going to do. (e.g. \"Test Zipf's Law\")\n\n\n\n\nThe first step is to count the frequency of each word in the book.\n\n\n./wordcount.py books/isles.txt isles.dat\n\n\n\n\nLet's take a quick peek at the result.\n\n\nhead -5 isles.dat\n\n\n\n\nshows us the top 5 lines in the output file:\n\n\nthe 3822    6.7371760973\nof  2460    4.33632998414\nand 1723    3.03719372466\nto  1479    2.60708619778\na   1308    2.30565838181\n\n\n\n\nEach row shows the word itself, the number of occurrences of that\nword, and the number of occurrences as a percentage of the total\nnumber of words in the text file.\n\n\nWe can do the same thing for a different book:\n\n\n./wordcount.py books/abyss.txt abyss.dat\nhead -5 abyss.dat\n\n\n\n\nFinally, let's visualize the results.\n\n\n./plotcount.py isles.dat ascii\n\n\n\n\nThe \nascii\n argument has been added so that we get a text-based\nbar-plot printed to the screen.\n\n\nThe script is also able to display a graphical bar-plot using matplotlib.\n\n\n./plotcount.py isles.dat show\n\n\n\n\nOr it can save the figure as a file.\n\n\n./plotcount.py isles.dat isles.png\n\n\n\n\nTogether these scripts implement a common workflow:\n\n\n\n\nRead a data file.\n\n\nPerform an analysis on this data file.\n\n\nWrite the analysis results to a new file.\n\n\nPlot a graph of the analysis results.\n\n\nSave the graph as an image, so we can put it in a paper.\n\n\n\n\nWriting a \"master\" script\n\n\nRunning this pipeline for one book is pretty easy using the command-line.\nBut once the number of files and the number of steps in the pipeline\nexpands, this can turn into a lot of work.\nPlus, no one wants to sit and wait for a command to finish, even just for 30\nseconds.\n\n\nThe most common solution to the tedium of data processing is to write\na master script that runs the whole pipeline from start to finish.\n\n\nWe can make a new file, \nrun_pipeline.sh\n that contains:\n\n\n#!/usr/bin/env bash\n# USAGE: bash run_pipeline.sh\n# to produce plots for isles and abyss.\n\n./wordcount.py isles.txt isles.dat\n./wordcount.py abyss.txt abyss.dat\n\n./plotcount.py isles.dat isles.png\n./plotcount.py abyss.dat abyss.png\n\n# Now archive the results in a tarball so we can share them with a colleague.\nrm -rf zipf_results\nmkdir zipf_results\nmv isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results\nrm -r zipf_results\n\n\n\n\nThis master script solved several problems in computational reproducibility:\n\n\n\n\nIt explicitly documents our pipeline,\n    making communication with colleagues (and our future selves) more efficient.\n\n\nIt allows us to type a single command, \nbash run_pipeline.sh\n, to\n    reproduce the full analysis.\n\n\nIt prevents us from \nrepeating\n typos or mistakes.\n    You might not get it right the first time, but once you fix something\n    it'll (probably) stay that way.\n\n\n\n\nTo continue with the Good Ideas, let's put everything under version control.\n\n\ngit init\ngit add README.md\ngit commit -m \"Starting a new project.\"\ngit add wordcount.py plotcount.py matplotlibrc\ngit commit -m \"Write scripts to test Zipf's law.\"\ngit add run_pipeline.sh\ngit commit -m \"Write a master script to run the pipeline.\"\n\n\n\n\nNotice that I didn't version control any of the products of our analysis.\nI'll talk more about this later.\n\n\nA master script is a good start, but it has a few shortcomings.\n\n\nLet's imagine that we adjusted the width of the bars in our plot\nproduced by \nplotcount.py\n.\n\n\nnano plotcount.py\n# In the definition of plot_word_counts replace:\n#    width = 1.0\n# with:\n#    width = 0.8\ngit add plotcount.py\ngit commit -m \"Fix the bar width.\"\n\n\n\n\nNow we want to recreate our figures.\nWe \ncould\n just \nbash run_pipeline.sh\n again.\nThat would work, but it could also be a big pain if counting words takes\nmore than a few seconds.\nThe word counting routine hasn't changed; we shouldn't need to recreate\nthose files.\n\n\nAlternatively, we could manually rerun the plotting for each word-count file\nand recreate the tarball.\n\n\nfor file in *.dat; do\n    ./plotcount.py $file ${file/.dat/.png}\ndone\n\nrm -rf zipf_results\nmkdir zipf_results\nmv isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results\nrm -r zipf_results\n\n\n\n\nBut then we don't get many of the benefits of having a master script in\nthe first place.\n\n\nAnother popular option is to comment out a subset of the lines in\n\nrun_pipeline.sh\n:\n\n\n#!/usr/bin/env bash\n# USAGE: bash run_pipeline.sh\n# to produce plots for isles and abyss.\n\n# These lines are commented out because they don't need to be rerun.\n#./wordcount.py isles.txt isles.dat\n#./wordcount.py abyss.txt abyss.dat\n\n./plotcount.py isles.dat isles.png\n./plotcount.py abyss.dat abyss.png\n\n# Now archive the results in a tarball so we can share them with a colleague.\nrm -rf zipf_results\nmkdir zipf_results\nmv isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results\nrm -r zipf_results\n\n\n\n\nFollowed by \nbash run_pipeline.sh\n.\n\n\nBut this process, and subsequently undoing it,\ncan be a hassle and source of errors in complicated pipelines.\n\n\nWhat we really want is an executable \ndescription\n of our pipeline that\nallows software to do the tricky part for us:\nfiguring out what steps need to be rerun.\nIt would also be nice if this tool encourage a \nmodular\n analysis\nand reusing instead of rewriting parts of our pipeline.\nAs an added benefit, we'd like it all to play nice with the other\nmainstays of reproducible research: version control, Unix-style tools,\nand a variety of scripting languages.\n\n\nMakefile basics [45 minutes]\n\n\nMake\n is a computer program originally designed to automate the compilation\nand installation of software.\n\nMake\n automates the process of building target files through a series of\ndiscrete steps.\nDespite it's original purpose, this design makes it a great fit for\nbioinformatics pipelines, which often work by transforming data from one form\nto another (e.g. \nraw data\n \u2192 \nword counts\n \u2192 \n???\n \u2192 \nprofit\n).\n\n\nFor this tutorial we will be using an implementation of \nMake\n called\n\nGNU Make\n, although others exist.\n\n\nA simple Makefile\n\n\nLet's get started writing a description of our analysis for \nMake\n.\n\n\nOpen up a file called \nMakefile\n in your editor of choice (e.g. \nnano Makefile\n)\nand add the following:\n\n\nisles.dat: books/isles.txt\n    ./wordcount.py books/isles.txt isles.dat\n\n\n\n\nWe have now written the simplest, non-trivial Makefile.\nIt is pretty reminiscent of one of the lines from our master script.\nIt is a good bet that you can figure out what this Makefile does.\n\n\nBe sure to notice a few syntactical items.\n\n\nThe part before the colon is called the \ntarget\n and the part after is our\nlist of \nprerequisites\n (there is just one in this case).\nThis first line is followed by an indented section called the \nrecipe\n.\nThe whole thing is together called a \nrule\n.\n\n\nNotice that the indent is \nnot\n multiple spaces, but is instead a single tab\ncharacter.\nThis is the first gotcha in makefiles.\nIf the difference between spaces and a tab character isn't obvious in your\neditor of choice, try moving your cursor from one side of the tab to the other.\nIt should \njump\n four or more spaces.\nIf your recipe is not indented with a tab character it is likely to not work.\n\n\nNotice that this recipe is exactly the same as the analogous step in our\nmaster shell script.\nThis is no coincidence; \nMake\n recipes \nare\n shell scripts.\nThe first line (\ntarget\n: \nprerequisites\n) explicitly declares two details\nthat were implicit in our pipeline script:\n\n\n\n\nWe are generating a file called \nisles.dat\n\n\nCreating this file requires \nbooks/isles.txt\n\n\n\n\nWe'll think about our pipeline as a network of files that are dependent\non one another.\nRight now our Makefile describes a pretty simple \ndependency graph\n.\n\n\n\n\nbooks/isles.txt\n \u2192 \nisles.dat\n\n\n\n\nwhere the \"\u2192\" is pointing from requirements to targets.\n\n\nDon't forget to commit:\n\n\ngit add Makefile\ngit commit -m \"Start converting master script into a Makefile.\"\n\n\n\n\nRunning \nMake\n\n\nNow that we have a (currently incomplete) description of our pipeline,\nlet's use \nMake\n to execute it.\n\n\nFirst, remove the previously generated files.\n\n\nrm *.dat *.png\n\n\n\n\nmake isles.dat\n\n\n\n\n\n\nAside\n\n\nNotice that we didn't tell \nMake\n to use \nMakefile\n.\nWhen you run \nmake\n, the program automatically looks in several places\nfor your Makefile.\nWhile other filenames will work,\nit is Good Idea to always call your Makefile \nMakefile\n.\n\n\n\n\nYou should see the following print to the terminal:\n\n\n./wordcount.py books/isles.txt isles.dat\n\n\n\n\nBy default, \nMake\n prints the recipes that it executes.\n\n\nLet's see if we got what we expected.\n\n\nhead -5 isles.dat\n\n\n\n\nThe first 5 lines of that file should look exactly like before.\n\n\nRerunning \nMake\n\n\nLet's try running \nMake\n the same way again.\n\n\nmake isles.dat\n\n\n\n\nThis time, instead of executing the same recipe,\n\nMake\n prints \nmake: Nothing to be done for 'isles.dat'.\n\n\nWhat's happening here?\n\n\nWhen you ask \nMake\n to make \nisles.dat\n it first looks at\nthe modification time of that target.\nNext it looks at the modification time for the target's prerequisites.\nIf the target is newer than the prerequisites \nMake\n decides that\nthe target is up-to-date and does not need to be remade.\n\n\nMuch has been said about using modification times as the cue for remaking\nfiles.\nThis can be another \nMake\n gotcha, so keep it in mind.\n\n\nIf you want to induce the original behavior, you just have to\nchange the modification time of \nbooks/isles.txt\n so that it is newer\nthan \nisles.dat\n.\n\n\ntouch books/isles.txt\nmake isles.dat\n\n\n\n\nThe original behavior is restored.\n\n\nSometimes you just want \nMake\n to tell you what it thinks about the current\nstate of your files.\n\nmake --dry-run isles.dat\n will print \nMake\n's execution plan, without\nactually carrying it out.\nThe flag can be abbreviated as \n-n\n.\n\n\nIf you don't pass a target as an argument to make (i.e. just run \nmake\n)\nit will assume that you want to build the first target in the Makefile.\n\n\nMore recipes\n\n\nNow that \nMake\n knows how to build \nisles.dat\n,\nwe can add a rule for plotting those results.\n\n\nisles.png: isles.dat\n    ./plotcount.py isles.dat isles.png\n\n\n\n\nThe dependency graph now looks like:\n\n\n\n\nbooks/isles.txt\n \u2192 \nisles.dat\n \u2192 \nisles.png\n\n\n\n\nLet's add a few more recipes to our Makefile.\n\n\nabyss.dat: books/abyss.txt\n    ./wordcount.py books/abyss.txt abyss.dat\n\nzipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp isles.dat abyss.dat isles.png abyss.png zipf_results/\n    tar -czf zipf_results.tgz zipf_results/\n    rm -r zipf_results/\n\n\n\n\nAnd commit the changes.\n\n\ngit add Makefile\ngit commit -m \"Add recipes for abyss counts, isles plotting, and the final archive.\"\n\n\n\n\nHere the recipe for \nzipf_results.tgz\n involves running a series of\nshell commands.\nWhen building the archive, \nMake\n will run each line successively unless\nany return an error.\n\n\n\n\nQuestion\n\n\nWithout doing it, what happens if you run \nmake isles.png\n?\n\n\nChallenge\n\n\nWhat does the dependency graph look like for your Makefile?\n\n\nTry it\n\n\nWhat happens if you run \nmake zipf_results.tgz\n right now?\n\n\nPractice\n\n\nWrite a recipe for \nabyss.png\n.\n\n\n\n\nOnce you've written a recipe for \nabyss.png\n you should be able to\nrun \nmake zipf_results.tgz\n.\n\n\nLet's delete all of our files and try it out.\n\n\nrm abyss.* isles.*\nmake zipf_results.tgz\n\n\n\n\nYou should get the something like the following output\n(the order may be different)\nto your terminal:\n\n\n./wordcount.py books/abyss.txt abyss.dat\n./wordcount.py books/isles.txt isles.dat\n./plotcount.py abyss.dat abyss.png\n./plotcount.py isles.dat isles.png\nrm -rf zipf_results/\nmkdir zipf_results/\ncp isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results/\nrm -r zipf_results/\n\n\n\n\nSince you asked for \nzipf_results.tgz\n \nMake\n looked first for that file.\nNot finding it, \nMake\n looked for its prerequisites.\nSince none of those existed it remade the ones it could,\n\nabyss.dat\n and \nisles.dat\n.\nOnce those were finished it was able to make \nabyss.png\n and\n\nisles.png\n, before finally building \nzipf_results.tgz\n.\n\n\n\n\nTry it\n\n\nWhat happens if you \ntouch abyss.dat\n and\nthen \nmake zipf_results.tgz\n?\n\n\n\n\ngit add Makefile\ngit commit -m \"Finish translating pipeline script to a Makefile.\"\ngit status\n\n\n\n\nNotice all the files that \nGit\n wants to be tracking?\nLike I said before, we're not going to version control any of the intermediate\nor final products of our pipeline.\nTo reflect this fact add a \n.gitignore\n file:\n\n\n*.dat\n*.png\nzipf_results.tgz\nLICENSE.md\n\n\n\n\ngit add .gitignore\ngit commit -m \"Have git ignore intermediate data files.\"\ngit status\n\n\n\n\nPhony targets\n\n\nSometimes its nice to have targets that don't refer to actual files.\n\n\nall: isles.png abyss.png zipf_results.tgz\n\n\n\n\nEven though this rule doesn't have a recipe, it does have prerequisites.\nNow, when you run \nmake all\n \nMake\n will do what it needs to to bring\nall three of those targets up to date.\n\n\nIt is traditional for \"\nall:\n\" to be the first recipe in a makefile,\nsince the first recipe is what is built by default\nwhen no other target is passed as an argument.\n\n\nAnother traditional target is \"\nclean\n\".\nAdd the following to your Makefile.\n\n\nclean:\n    rm --force *.dat *.png zipf_results.tgz\n\n\n\n\nRunning \nmake clean\n will now remove all of the cruft.\n\n\nWatch out, though!\n\n\n\n\nTry it\n\n\nWhat happens if you create a file named \nclean\n (i.e. \ntouch clean\n)\nand then run \nmake clean\n?\n\n\n\n\nWhen you run \nmake clean\n you get \nmake: Nothing to be done for 'clean'.\n.\nThat's \nnot\n because all those files have already been removed.\n\nMake\n isn't that smart.\nInstead, make sees that there is already a file named \"\nclean\n\" and,\nsince this file is newer than all of its prerequisites (there are none),\n\nMake\n decides there's nothing left to do.\n\n\nTo avoid this problem add the following to your Makefile.\n\n\n.PHONY: all clean\n\n\n\n\nThis \"special target\" tells \nMake\n to assume that the targets \"all\", and \"clean\"\nare \nnot\n real files;\nthey're \nphony\n targets.\n\n\ngit add Makefile\ngit commit -m \"Added 'all' and 'clean' recipes.\"\n\n\n\n\nMake\n features [45 minutes]\n\n\nRight now our Makefile looks like this:\n\n\n# Dummy targets\nall: isles.png abyss.png zipf_results.tgz\n\nclean:\n    rm --force *.dat *.png zipf_results.tgz\n\n.PHONY: all clean\n\n# Analysis and plotting\nisles.dat: books/isles.txt\n    ./wordcount.py books/isles.txt isles.dat\n\nisles.png: isles.dat\n    ./plotcount.py isles.dat isles.png\n\nabyss.dat: books/abyss.txt\n    ./wordcount.py books/abyss.txt abyss.dat\n\nabyss.png: abyss.png\n    ./plotcount.py abyss.dat abyss.png\n\n# Archive for sharing\nzipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp isles.dat abyss.dat isles.png abyss.png zipf_results/\n    tar -czf zipf_results.tgz zipf_results/\n    rm -r zipf_results/\n\n\n\n\nLooks good, don't you think?\nNotice the added comments, starting with the \"\n#\n\" character just like in\nPython, R, shell, etc.\n\n\nUsing these recipes, a simple call to \nmake\n builds all the same files that\nwe were originally making either manually or using the master script,\nbut with a few bonus features.\n\n\nNow, if we change one of the inputs, we don't have to rebuild everything.\nInstead, \nMake\n knows to only rebuild the files that, either directly or\nindirectly, depend on the file that changed.\nThis is called an \nincremental build\n.\nIt's no longer our job to track those dependencies.\nOne fewer cognitive burden getting in the way of research progress!\n\n\nIn addition, a makefile explicitly documents the inputs to and outputs\nfrom every step in the analysis.\nThese are like informal \"USAGE:\" documentation for our scripts.\n\n\nParallel \nMake\n\n\nAnd check this out!\n\n\nmake clean\nmake --jobs\n\n\n\n\nDid you see it?\nThe \n--jobs\n flag (just \n-j\n works too) tells \nMake\n to run recipes in \nparallel\n.\nOur dependency graph clearly shows that\n\nabyss.dat\n and \nisles.dat\n are mutually independent and can\nboth be built at the same time.\nLikewise for \nabyss.png\n and \nisles.png\n.\nIf you've got a bunch of independent branches in your analysis, this can\ngreatly speed up your build process.\n\n\nD.R.Y. (Don't Repeat Yourself)\n\n\nIn many programming language, the bulk of the language features are there\nto allow the programmer to describe long-winded computational routines as\nshort, expressive, beautiful code.\nFeatures in Python or R like user-defined variables and functions are\nuseful in part because they mean we don't have to write out (or think about)\nall of the details over and over again.\nThis good habit of writing things out only once is known as the D.R.Y.\nprinciple.\n\n\nIn \nMake\n a number of features are designed to minimize repetitive code.\nOur current makefile does \nnot\n conform to this principle,\nbut \nMake\n is perfectly capable of solving the problem.\n\n\nAutomatic variables\n\n\nOne overly repetitive part of our Makefile:\nTargets and prerequisites are in both the header \nand\n the recipe of each rule.\n\n\nIt turns out, that\n\n\nisles.dat: books/isles.txt\n    ./wordcount.py books/isles.txt isles.dat\n\n\n\n\ncan be rewritten as\n\n\nisles.dat: books/isles.txt\n    ./wordcount.py $^ $@\n\n\n\n\nHere we've replaced the prerequisite \"\nbooks/isles.txt\n\" in the recipe\nwith \"\n$^\n\" and the target \"\nisles.dat\n\" with \"\n$@\n\".\nBoth \"\n$^\n\" and \"\n$@\n\" are variables that refer to all of the prerequisites and\ntarget of a rule, respectively.\nIn \nMake\n, variables are referenced with a leading dollar sign symbol.\nWhile we can also define our own variables,\n\nMake\n \nautomatically\n defines a number of variables, including each of these.\n\n\nzipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp isles.dat abyss.dat isles.png abyss.png zipf_results/\n    tar -czf zipf_results.tgz zipf_results/\n    rm -r zipf_results/\n\n\n\n\ncan now be rewritten as\n\n\nzipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp $^ zipf_results/\n    tar -czf $@ zipf_results/\n    rm -r zipf_results/\n\n\n\n\nPhew!  That's much less cluttered,\nand still perfectly understandable once you know what the variables mean.\n\n\n\n\nTry it\n\n\n```bash\nmake clean\nmake isles.dat\n``````````\n\n\n\n\n\nYou should get the same output as last time.\nInternally, \nMake\n replaced \"\n$@\n\" with \"\nisles.dat\n\"\nand \"\n$^\n\" with \"\nbooks/isles.txt\n\"\nbefore running the recipe.\n\n\n\n\nPractice\n\n\nGo ahead and rewrite all of the rules in your Makefile to minimize\nrepetition and take advantage of these automatic variables.\nDon't forget to commit your work.\n\n\n\n\nPattern rules\n\n\nAnother deviation from D.R.Y.:\nWe have nearly identical recipes for \nabyss.dat\n and \nisles.dat\n.\n\n\nIt turns out we can replace \nboth\n of those rules with just one rule,\nby telling \nMake\n about the relationships between filename \npatterns\n.\n\n\nA \"pattern rule\" looks like this:\n\n\n%.dat: books/%.txt\n    countwords.py $^ $@\n\n\n\n\nHere we've replaced the book name with a percent sign, \"\n%\n\".\nThe \"\n%\n\" is called the \nstem\n\nand matches any sequence of characters in the target.\n(Kind of like a \"\n*\n\" (glob) in a path name, but they are \nnot\n the same.)\nWhatever it matches is then filled in to the prerequisites\nwherever there's a \"\n%\n\".\n\n\nThis rule can be interpreted as:\n\n\n\n\nIn order to build a file named \n[something].dat\n (the target)\nfind a file named \nbooks/[that same something].txt\n (the prerequisite)\nand run \ncountwords.py [the prerequisite] [the target]\n.\n\n\n\n\nNotice how helpful the automatic variables are here.\nThis recipe will work no matter what stem is being matched!\n\n\nWe can replace \nboth\n of the rules that matched this pattern\n(\nabyss.dat\n and \nisles.dat\n) with just one rule.\nGo ahead and do that in your Makefile.\n\n\n\n\nTry it\n\n\nAfter you've replaced the two rules with one pattern\nrule, try removing all of the products (\nmake clean\n)\nand rerunning the pipeline.\n\n\nIs anything different now that you're using the pattern rule?\n\n\nIf everything still works, commit your changes to \nGit\n.\n\n\nPractice\n\n\nReplace the recipes for \nabyss.png\n and \nisles.png\n\nwith a single pattern rule.\n\n\nChallenge\n\n\nAdd \nbooks/sierra.txt\n to your pipeline.\n\n\n(i.e. \nmake all\n should plot the word counts and add the plots to\n\nzipf_results.tgz\n)\n\n\n\n\nCommit your changes to \nGit\n before we move on.\n\n\nUser defined variables\n\n\nNot all variables in a makefile are of the automatic variety.\nUsers can define their own, as well.\n\n\nAdd this lines at the top of your makefile:\n\n\nARCHIVED := isles.dat isles.png \\\n            abyss.dat abyss.png \\\n            sierra.dat sierra.png\n\n\n\n\nJust like many other languages,\nin makefiles \"\n\\\n\" is a line-continuation character.\nThink of this variable definition as a single line without the backslash.\n\n\nThe variable \nARCHIVED\n is a list of the files that we want to include in our\ntarball.\nNow wherever we write \n${ARCHIVED}\n it will be replaced with that list of files.\nThe dollar sign, \"\n$\n\", and curly-braces, \"\n{}\n\", are both mandatory when\ninserting the contents of a variable.\n\n\nNotice the backslashes in the variable definition\nsplitting the list over three lines, instead of one very long line.\nAlso notice that we assigned to the variable with \"\n:=\n\".\nThis is generally a Good Idea;\nAssigning with a normal equals sign can result in non-intuitive behavior\n(for reasons we may not talk about).\nFinally, notice that the items in our list are separated by \nwhitespace\n,\nnot commas.\nPrerequisite lists were the same way; this is just how lists of things work in\nmakefiles.\nIf you included commas they would be considered parts of the filenames.\n\n\nUsing this variable we can replace the prerequisites of \nzipf_results.tgz\n.\nThat rule would now be:\n\n\nzipf_results.tgz: ${ARCHIVED}\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp $^ zipf_results/\n    tar -czf $@ zipf_results/\n    rm -r zipf_results/\n\n\n\n\nWe can also use \n${ARCHIVED}\n to simplify our cleanup rule.\n\n\nclean:\n    rm --force ${ARCHIVED} zipf_results.tgz\n\n\n\n\n\n\nTry it\n\n\nTry running \nclean\n and then \nall\n.\n\n\nDoes everything still work?\n\n\n\n\nBest practices for \nMake\n-based projects [60 minutes]\n\n\nA Makefile can be an important part of a reproducible research pipeline.\nHave you noticed how simple it is now to add/remove books from our analysis?\nJust add or remove those files from the definition of \nARCHIVED\n or\nthe prerequisites for the \nall\n target!\nWith the master script \nrun_pipeline.sh\n,\nadding a third book required either more complicated\nor less transparent changes.\n\n\nWhat's a prerequisite?\n\n\nWe've talked a lot about the power of \nMake\n for\nrebuilding research outputs when input data changes.\nWhen doing novel data analysis, however, it's very common for our \nscripts\n to\nbe as or \nmore\n dynamic than the data.\n\n\nWhat happens when we edit our scripts instead of changing our data?\n\n\n\n\nTry it\n\n\nFirst, run \nmake all\n so your analysis is up-to-date.\n\n\nLet's change the default number of entries in the rank/frequency\nplot from 10 to 5.\n\n\n(Hint: edit the function definition for \nplot_word_counts\n in\n\nplotcounts.py\n to read \nlimit=5\n.)\n\n\nNow run \nmake all\n again.  What happened?\n\n\n\n\nAs it stands, we have to run \nmake clean\n followed by \nmake all\n\nto update our analysis with the new script.\nWe're missing out on the benefits of incremental analysis when our scripts\nare changing too.\n\n\nThere must be a better way...and there is!  Scripts should be prerequisites too.\n\n\nLet's edit the pattern rule for \n%.png\n to include \nplotcounts.py\n\nas a prerequisites.\n\n\n%.png: plotcounts.py %.dat\n    $^ $@\n\n\n\n\nThe header makes sense, but that's a strange looking recipe:\njust two automatic variables.\n\n\nThis recipe works because \"\n$^\n\" is replaced with all of the prerequisites.\n\nIn order\n.\nWhen building \nabyss.png\n, for instance, it is replaced with\n\nplotcounts.py abyss.dat\n, which is actually exactly what we want.\n\n\n\n\nTry it\n\n\nWhat happens when you run the pipeline after modifying your script again?\n\n\n(Changes to your script can be simulated with \ntouch plotcounts.py\n.)\n\n\nPractice\n\n\nUpdate your other rules to include the relevant scripts as prerequisites.\n\n\nCommit your changes.\n\n\n\n\nDirectory structure\n\n\nTake a look at all of the clutter in your project directory (run \nls\n to\nlist all of the files).\nFor such a small project that's a lot of junk!\nImagine how hard it would be to find your way around this analysis\nif you had more than three steps?\nLet's move some stuff around to make our project easier to navigate.\n\n\nStore scripts in \nscripts/\n\n\nFirst we'll stow away the scripts.\n\n\nmkdir scripts/\nmv plotcounts.py wordcount.py scripts/\n\n\n\n\nWe also need to update our Makefile to reflect the change:\n\n\n%.dat: countwords.py books/%.txt\n    $^ $@\n\n%.png: plotcounts.py %.dat\n    $^ $@\n\n\n\n\nbecomes:\n\n\n%.dat: scripts/countwords.py books/%.txt\n    $^ $@\n\n%.png: scripts/plotcounts.py %.dat\n    $^ $@\n\n\n\n\nThat's a little more verbose, but it is now explicit\nthat \ncountwords.py\n and \nplotcount.py\n are scripts.\n\n\nGit\n should have no problem with the move once you tell it which files\nto be aware of.\n\n\ngit add countwords.py plotcounts.py\ngit add scripts/countwords.py scripts/plotcounts.py\ngit add Makefile\ngit commit -m \"Move scripts into a subdirectory.\"\n\n\n\n\nGreat!  From here on, when we add new scripts to our analysis they won't\nclutter up our project root.\n\n\n\"Hide\" intermediate files in \ndata/\n\n\nSpeaking of clutter, what are we gonna do about all of these intermediate files!?\nPut 'em in a subdirectory!\n\n\nmkdir data/\nmv *.tsv data/\n\n\n\n\nAnd then fix up your Makefile.\nAdjust the relevant lines to look like this.\n\n\n# ...\n\nARCHIVED := data/isles.dat isles.png \\\n            data/abyss.dat abyss.png \\\n            data/sierra.dat sierra.png\n\n# ...\n\ndata/%.dat: scripts/countwords.py books/%.txt\n    $^ $@\n\n%.png: scripts/plotcounts.py data/%.dat\n\n# ...\n\n\n\n\nThanks to our \nARCHIVED\n variable, making these changes is pretty simple.\n\n\nWe have to make one more change if we don't want \nGit\n to bother us about\nuntracked files.\nUpdate your \n.gitignore\n.\n\n\ndata/*.dat\n*.png\nzipf_results.tgz\nLICENSE.md\n\n\n\n\nNow commit your changes.\n\n\ngit add Makefile\ngit add .gitignore\n\n\n\n\nSimple!\n\n\nOutput finished products to \nfig/\n\n\n\n\nPractice\n\n\nMove the plots and \nzipf_results.tgz\n to a directory called \nfig/\n.\n\n\n\n\nYou can call this directory something else if you prefer, but \nfig/\n seems\nshort and descriptive.\n\n\n\n\nTry it\n\n\nDoes your pipeline still execute the way you expect?\n\n\n\n\nFile naming\n\n\nUse file extensions to indicate format\n\n\nUp to this point, we've been working with three types of data files,\neach with it's own file extension.\n\n\n\n\n'\n.txt\n' files: the original book in plain-text\n\n\n'\n.dat\n' files: word counts and percentages in a plain-text format\n\n\n'\n.png\n' files: PNG formatted barplots\n\n\n\n\nUsing file extensions like these clearly indicates to anyone not familiar with\nyour project what software to view each file with;\nyou won't get much out of opening a PNG with a text editor.\nWhenever possible, use a widely used extension to make it easy for others\nto understand your data.\n\n\nFile extensions also give us a handle for describing the flow of data in our\npipeline.\nPattern rules rely on this convention.\nOur makefile says that the raw, book data feeds into word count data\nwhich feeds into barplot data.\n\n\nBut the current naming scheme has one obvious ambiguity:\n'\n.dat\n' isn't particularly descriptive.\nLots of file formats can be described as \"data\", including binary formats\nthat would require specialized software to view.\nFor tab-delimited, tabular data (data in rows and columns),\n'\n.tsv\n' is a more precise convention.\n\n\nUpdating our pipeline to use this extension is as simple as find-and-replace\n'\n.dat\n' to '\n.tsv\n' in our Makefile.\nIf you're tired of \nmv\n-ing your files every time you change your pipeline\nyou can also \nmake clean\n followed by \nmake all\n to check that everything still\nworks.\n\n\nYou might want to update your \"\nclean\n\" recipe to remove all the junk\nlike so:\n\n\nclean:\n    rm -f data/* fig/*\n\n\n\n\nBe sure to commit all of your changes.\n\n\nInfix processing hints\n\n\nOne of our goals in implementing best practices for our analysis pipeline\nis to make it easy to change it without rewriting everything.\nLet's add a preprocessing step to our analysis that puts\neverything in lowercase before counting words.\n\n\nThe program \ntr\n (short for \"translate\") is a Unix-style filter that swaps one\nset of characters for another.\n\ntr '[:upper:]' '[:lower:]' < [input file] > [output file]\n\nwill read the mixedcase input file and write all lowercase to\nthe output file.\n\n\nWe can add this to our pipeline by first adding a rule.\nWe know the recipe is going to look like this:\n\n\ntr '[:upper:]' '[:lower:]' < $^ > $@\n\n\n\n\n\n\nChallenge\n\n\nRewrite your Makefile to update the pipeline with the preprocessing step.\n\n\n\n\nYou probably decided to take the pattern \nbooks/%.txt\n as the prerequisite,\nbut what did you opt to name the target?\n\n\ndata/%.txt\n is an option, but that means we have two files named\n\n[bookname].txt\n, one in \nbooks/\n and one in \ndata/\n.\nProbably not the easiest to differentiate.\n\n\nA better option is to use a more descriptive filename.\n\n\ndata/%.lower.txt: books/%.txt\n    tr '[:upper:]' '[:lower:]' < $^ > $@\n\n\n\n\nBy including an \ninfix\n of \n.lower.\n in our filename it's easy to\nsee that one file is a lowercase version of the mixedcase original.\nNow we can extend our pipeline with a variety of pre- and post-processing\nsteps, give each of them a descriptive infix,\nand the names will be a self-documenting record of its origins.\n\n\nFor reasons which will be explained in a minute, let's also make a dummy\npreprocessing step which will just copy the books verbatim into our\n\ndata/\n directory.\n\n\ndata/%.txt: books/%.txt\n    cp $^ $@\n\n\n\n\nAnd, in the spirit of infixes, we'll rename \ndata/%.tsv\n to be more descriptive.\n\n\ndata/%.counts.tsv: scripts/wordcount.py data/%.txt\n    $^ $@\n\nfig/%.counts.png: scripts/plotcount.py data/%.counts.tsv\n    $^ $@\n\n\n\n\nHere's the \nfull\n Makefile:\n\n\nARCHIVED := data/isles.lower.counts.tsv data/abyss.lower.counts.tsv \\\n            data/sierra.lower.counts.tsv fig/isles.lower.counts.png \\\n            fig/abyss.lower.counts.png fig/sierra.lower.counts.png\n\n# Dummy targets\nall: fig/isles.lower.counts.png fig/abyss.lower.counts.png \\\n        fig/sierra.lower.counts.png zipf_results.tgz\n\nclean:\n    rm --force data/* fig/*\n\n.PHONY: all clean\n\n# Analysis and plotting\ndata/%.txt: books/%.txt\n    cp $^ $@\n\ndata/%.lower.txt: data/%.txt\n    tr '[:upper:]' '[:lower:]' < $^ > $@\n\ndata/%.counts.tsv: scripts/wordcount.py data/%.txt\n    $^ $@\n\nfig/%.counts.png: scripts/plotcount.py data/%.counts.tsv\n    $^ $@\n\n# Archive for sharing\nzipf_results.tgz: ${ARCHIVED}\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp $^ zipf_results/\n    tar -czf $@ zipf_results/\n    rm -r zipf_results/\n\n\n\n\nOur filenames are certainly more verbose now, but in exchange we get:\n\n\n\n\nself-documenting filenames\n\n\nmore flexible development\n\n\nand something else, too...\n\n\n\n\nmake clean\nmake fig/abyss.lower.counts.png fig/abyss.counts.png\n\n\n\n\nWhat happened there?\nWe just built two different barplots, one for our analysis \nwith\n the\npreprocessing step and one \nwithout\n.\nBoth from the same Makefile.\nBy liberally applying pattern rules and infix filenames\nwe get something like a \"filename language\".\nWe describe the analyses we want to run and then have \nMake\n figure out the\ndetails.\n\n\n\n\nPractice\n\n\nUpdate your drawing of the dependency graph.\n\n\n\n\nBuilt-in Testing\n\n\nIt's a Good Idea to check your analysis against some form of ground truth.\nThe simplest version of this is a well-defined dataset that you can\nreason about independent of your code.\nLet's make just such a dataset.\nLet's write a book!\n\n\nInto a file called \nbooks/test.txt\n add something like this:\n\n\nMy Book\nBy Me\n\nThis is a book that I wrote.\n\nThe END\n\n\n\n\n\nWe don't need software to count all of the words in this book, and\nwe can probably imagine exactly what a barplot of the count would look like.\nIf the actual result doesn't look like we expected,\nthen there's probably something wrong with our analysis.\nTesting your scripts with this tiny book is computationally cheap, too.\n\n\nLet's try it out!\n\n\nmake fig/test.lower.counts.png\nless data/test.lower.counts.tsv\n\n\n\n\nDoes your counts data match what you expected?\n\n\nWe should run this test for just about every change we make,\nto our scripts or to our Makefile.\nWe're going to do that a \nlot\n so we'll make it as easy as possible.\n\n\ntest: fig/test.lower.counts.png\n\n.PHONY: test clean all\n\n\n\n\nYou could even add the \ntest\n phony target as the first thing in your Makefile.\nThat way just calling \nmake\n will run your tests.\n\n\n\n\nPractice\n\n\nAdd a cleanup target called \ntestclean\n which is specific for\nthe outputs of your test run.\n\n\n\n\nCommit your changes, including \nbooks/test.txt\n.\n\n\ngit add Makefile\ngit add -f books/test.txt\ngit commit -m \"Add pipeline testing recipe and book.\"\n\n\n\n\nReview: version control\n\n\nWe have been following three guiding principles in our use of version\ncontrol during this lesson.\n\n\n\n\n\n\nUse it (always).\n\n\nVersion control is a Good Idea and should be used for any files which\ndescribe your pipeline.\nThis includes notes/documentation/TODOs, scripts, and the Makefiles\nthemselves.\n\n\n\n\n\n\nDon't version control raw or processed data which can be recreated.\n\n\nRaw data stays raw and data cleanup should be part of the pipeline.\nBecause of this, backing up your data is imperative, but version\ncontrol is not usually the best way to do so.\nConsider adding a recipe which downloads raw data using\n\nwget\n or \ncurl\n.\n\n\nOne exception would be test or example data.\nThese should be version controlled, as they are subject to change\nas testing is adapted to the evolving pipeline.\n\n\nMetadata should also be version controlled, since the format and\ncomposition of the metadata is intimately linked with the analysis pipeline\nitself.\n\n\n\n\n\n\nAim to commit \"atomic\" changes to your pipeline.\n\n\nThis means you should usually run \nmake test\n before committing\nyour changes so that regressions don't need to be fixed\nin subsequent commits.\nCo-dependent updates to metadata, documentation, and testing should\nbe included in the same commit.\nIn a perfect world, \nmake all\n should work, and documentation\nshould be up to date, regardless of what revision has been checked out.\nExcessive application of this principle is ill advised.\n\n\nA more common problem are behemoth commits which make large numbers of\nunrelated changes.\nIn general, a single sentence commit message should be able to summarize\nall of the changes in a commit.\n\n\n\n\n\n\nExercises left to the reader\n\n\n\n\nChallenge\n\n\nDownload your data using \nMake\n.\n\n\nChallenge\n\n\nBootstrap your local setup using \nMake\n.",
            "title": "Home"
        },
        {
            "location": "/#setup-15-minutes",
            "text": "This tutorial is designed to be run on Amazon EC2 using the\nUbuntu Server 14.04 LTS image, although it should be trivial to port\nit for use on any other UNIX operating system.\nI've tested that everything works on an m3.medium instance.\nIf you would like to use Windows, Git-Bash (packaged with Git for Windows)\nis probably your best bet, although it has not been tested on that platform.  For this lesson we will be using an already prepared set of files.  curl https://codeload.github.com/bsmith89/make-example/tar.gz/master \\\n    > make-example-master.tgz\ntar -xzf make-example-master.tgz\ncd make-example-master  Let's take a look at the files we will be working with:  sudo apt-get update\nsudo apt-get install tree\ntree  The  tree  command produces a handy tree-diagram of the directory.  .\n\u251c\u2500\u2500 books\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 abyss.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 isles.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 last.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE_TEXTS.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sierra.txt\n\u251c\u2500\u2500 LICENSE.md\n\u251c\u2500\u2500 matplotlibrc\n\u251c\u2500\u2500 plotcount.py\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 wordcount.py\n\n1 directory, 7 files  Be sure that you also have  Python 3 ,  Git , and  GNU Make .  sudo apt-get install git make  Configure git.  git config --global user.name \"Your Name\"\ngit config --global user.email you@example.com  Install matplotlib.  sudo apt-get install python3-matplotlib  And, finally, load up your favorite terminal multi-plexer so we can recover if\nwe get disconnected.  tmux",
            "title": "Setup [15 minutes]"
        },
        {
            "location": "/#motivation-30-minutes",
            "text": "The most frequently-occurring word occurs approximately twice as\noften as the second most frequent word. This is Zipf's Law .   Let's imagine that instead of computational biology we're interested in\ntesting Zipf's law in some of our favorite books.\nWe've compiled our raw data, the books we want to analyze\n(check out  head books/isles.txt )\nand have prepared several Python scripts that together make up our\nanalysis pipeline.  Before we begin, add a README to your project describing what we intend\nto do.  nano README.md\n# Describe what you're going to do. (e.g. \"Test Zipf's Law\")  The first step is to count the frequency of each word in the book.  ./wordcount.py books/isles.txt isles.dat  Let's take a quick peek at the result.  head -5 isles.dat  shows us the top 5 lines in the output file:  the 3822    6.7371760973\nof  2460    4.33632998414\nand 1723    3.03719372466\nto  1479    2.60708619778\na   1308    2.30565838181  Each row shows the word itself, the number of occurrences of that\nword, and the number of occurrences as a percentage of the total\nnumber of words in the text file.  We can do the same thing for a different book:  ./wordcount.py books/abyss.txt abyss.dat\nhead -5 abyss.dat  Finally, let's visualize the results.  ./plotcount.py isles.dat ascii  The  ascii  argument has been added so that we get a text-based\nbar-plot printed to the screen.  The script is also able to display a graphical bar-plot using matplotlib.  ./plotcount.py isles.dat show  Or it can save the figure as a file.  ./plotcount.py isles.dat isles.png  Together these scripts implement a common workflow:   Read a data file.  Perform an analysis on this data file.  Write the analysis results to a new file.  Plot a graph of the analysis results.  Save the graph as an image, so we can put it in a paper.",
            "title": "Motivation [30 minutes]"
        },
        {
            "location": "/#writing-a-master-script",
            "text": "Running this pipeline for one book is pretty easy using the command-line.\nBut once the number of files and the number of steps in the pipeline\nexpands, this can turn into a lot of work.\nPlus, no one wants to sit and wait for a command to finish, even just for 30\nseconds.  The most common solution to the tedium of data processing is to write\na master script that runs the whole pipeline from start to finish.  We can make a new file,  run_pipeline.sh  that contains:  #!/usr/bin/env bash\n# USAGE: bash run_pipeline.sh\n# to produce plots for isles and abyss.\n\n./wordcount.py isles.txt isles.dat\n./wordcount.py abyss.txt abyss.dat\n\n./plotcount.py isles.dat isles.png\n./plotcount.py abyss.dat abyss.png\n\n# Now archive the results in a tarball so we can share them with a colleague.\nrm -rf zipf_results\nmkdir zipf_results\nmv isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results\nrm -r zipf_results  This master script solved several problems in computational reproducibility:   It explicitly documents our pipeline,\n    making communication with colleagues (and our future selves) more efficient.  It allows us to type a single command,  bash run_pipeline.sh , to\n    reproduce the full analysis.  It prevents us from  repeating  typos or mistakes.\n    You might not get it right the first time, but once you fix something\n    it'll (probably) stay that way.   To continue with the Good Ideas, let's put everything under version control.  git init\ngit add README.md\ngit commit -m \"Starting a new project.\"\ngit add wordcount.py plotcount.py matplotlibrc\ngit commit -m \"Write scripts to test Zipf's law.\"\ngit add run_pipeline.sh\ngit commit -m \"Write a master script to run the pipeline.\"  Notice that I didn't version control any of the products of our analysis.\nI'll talk more about this later.  A master script is a good start, but it has a few shortcomings.  Let's imagine that we adjusted the width of the bars in our plot\nproduced by  plotcount.py .  nano plotcount.py\n# In the definition of plot_word_counts replace:\n#    width = 1.0\n# with:\n#    width = 0.8\ngit add plotcount.py\ngit commit -m \"Fix the bar width.\"  Now we want to recreate our figures.\nWe  could  just  bash run_pipeline.sh  again.\nThat would work, but it could also be a big pain if counting words takes\nmore than a few seconds.\nThe word counting routine hasn't changed; we shouldn't need to recreate\nthose files.  Alternatively, we could manually rerun the plotting for each word-count file\nand recreate the tarball.  for file in *.dat; do\n    ./plotcount.py $file ${file/.dat/.png}\ndone\n\nrm -rf zipf_results\nmkdir zipf_results\nmv isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results\nrm -r zipf_results  But then we don't get many of the benefits of having a master script in\nthe first place.  Another popular option is to comment out a subset of the lines in run_pipeline.sh :  #!/usr/bin/env bash\n# USAGE: bash run_pipeline.sh\n# to produce plots for isles and abyss.\n\n# These lines are commented out because they don't need to be rerun.\n#./wordcount.py isles.txt isles.dat\n#./wordcount.py abyss.txt abyss.dat\n\n./plotcount.py isles.dat isles.png\n./plotcount.py abyss.dat abyss.png\n\n# Now archive the results in a tarball so we can share them with a colleague.\nrm -rf zipf_results\nmkdir zipf_results\nmv isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results\nrm -r zipf_results  Followed by  bash run_pipeline.sh .  But this process, and subsequently undoing it,\ncan be a hassle and source of errors in complicated pipelines.  What we really want is an executable  description  of our pipeline that\nallows software to do the tricky part for us:\nfiguring out what steps need to be rerun.\nIt would also be nice if this tool encourage a  modular  analysis\nand reusing instead of rewriting parts of our pipeline.\nAs an added benefit, we'd like it all to play nice with the other\nmainstays of reproducible research: version control, Unix-style tools,\nand a variety of scripting languages.",
            "title": "Writing a \"master\" script"
        },
        {
            "location": "/#makefile-basics-45-minutes",
            "text": "Make  is a computer program originally designed to automate the compilation\nand installation of software. Make  automates the process of building target files through a series of\ndiscrete steps.\nDespite it's original purpose, this design makes it a great fit for\nbioinformatics pipelines, which often work by transforming data from one form\nto another (e.g.  raw data  \u2192  word counts  \u2192  ???  \u2192  profit ).  For this tutorial we will be using an implementation of  Make  called GNU Make , although others exist.",
            "title": "Makefile basics [45 minutes]"
        },
        {
            "location": "/#a-simple-makefile",
            "text": "Let's get started writing a description of our analysis for  Make .  Open up a file called  Makefile  in your editor of choice (e.g.  nano Makefile )\nand add the following:  isles.dat: books/isles.txt\n    ./wordcount.py books/isles.txt isles.dat  We have now written the simplest, non-trivial Makefile.\nIt is pretty reminiscent of one of the lines from our master script.\nIt is a good bet that you can figure out what this Makefile does.  Be sure to notice a few syntactical items.  The part before the colon is called the  target  and the part after is our\nlist of  prerequisites  (there is just one in this case).\nThis first line is followed by an indented section called the  recipe .\nThe whole thing is together called a  rule .  Notice that the indent is  not  multiple spaces, but is instead a single tab\ncharacter.\nThis is the first gotcha in makefiles.\nIf the difference between spaces and a tab character isn't obvious in your\neditor of choice, try moving your cursor from one side of the tab to the other.\nIt should  jump  four or more spaces.\nIf your recipe is not indented with a tab character it is likely to not work.  Notice that this recipe is exactly the same as the analogous step in our\nmaster shell script.\nThis is no coincidence;  Make  recipes  are  shell scripts.\nThe first line ( target :  prerequisites ) explicitly declares two details\nthat were implicit in our pipeline script:   We are generating a file called  isles.dat  Creating this file requires  books/isles.txt   We'll think about our pipeline as a network of files that are dependent\non one another.\nRight now our Makefile describes a pretty simple  dependency graph .   books/isles.txt  \u2192  isles.dat   where the \"\u2192\" is pointing from requirements to targets.  Don't forget to commit:  git add Makefile\ngit commit -m \"Start converting master script into a Makefile.\"",
            "title": "A simple Makefile"
        },
        {
            "location": "/#running-make",
            "text": "Now that we have a (currently incomplete) description of our pipeline,\nlet's use  Make  to execute it.  First, remove the previously generated files.  rm *.dat *.png  make isles.dat",
            "title": "Running Make"
        },
        {
            "location": "/#aside",
            "text": "Notice that we didn't tell  Make  to use  Makefile .\nWhen you run  make , the program automatically looks in several places\nfor your Makefile.\nWhile other filenames will work,\nit is Good Idea to always call your Makefile  Makefile .   You should see the following print to the terminal:  ./wordcount.py books/isles.txt isles.dat  By default,  Make  prints the recipes that it executes.  Let's see if we got what we expected.  head -5 isles.dat  The first 5 lines of that file should look exactly like before.",
            "title": "Aside"
        },
        {
            "location": "/#rerunning-make",
            "text": "Let's try running  Make  the same way again.  make isles.dat  This time, instead of executing the same recipe, Make  prints  make: Nothing to be done for 'isles.dat'.  What's happening here?  When you ask  Make  to make  isles.dat  it first looks at\nthe modification time of that target.\nNext it looks at the modification time for the target's prerequisites.\nIf the target is newer than the prerequisites  Make  decides that\nthe target is up-to-date and does not need to be remade.  Much has been said about using modification times as the cue for remaking\nfiles.\nThis can be another  Make  gotcha, so keep it in mind.  If you want to induce the original behavior, you just have to\nchange the modification time of  books/isles.txt  so that it is newer\nthan  isles.dat .  touch books/isles.txt\nmake isles.dat  The original behavior is restored.  Sometimes you just want  Make  to tell you what it thinks about the current\nstate of your files. make --dry-run isles.dat  will print  Make 's execution plan, without\nactually carrying it out.\nThe flag can be abbreviated as  -n .  If you don't pass a target as an argument to make (i.e. just run  make )\nit will assume that you want to build the first target in the Makefile.",
            "title": "Rerunning Make"
        },
        {
            "location": "/#more-recipes",
            "text": "Now that  Make  knows how to build  isles.dat ,\nwe can add a rule for plotting those results.  isles.png: isles.dat\n    ./plotcount.py isles.dat isles.png  The dependency graph now looks like:   books/isles.txt  \u2192  isles.dat  \u2192  isles.png   Let's add a few more recipes to our Makefile.  abyss.dat: books/abyss.txt\n    ./wordcount.py books/abyss.txt abyss.dat\n\nzipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp isles.dat abyss.dat isles.png abyss.png zipf_results/\n    tar -czf zipf_results.tgz zipf_results/\n    rm -r zipf_results/  And commit the changes.  git add Makefile\ngit commit -m \"Add recipes for abyss counts, isles plotting, and the final archive.\"  Here the recipe for  zipf_results.tgz  involves running a series of\nshell commands.\nWhen building the archive,  Make  will run each line successively unless\nany return an error.",
            "title": "More recipes"
        },
        {
            "location": "/#question",
            "text": "Without doing it, what happens if you run  make isles.png ?",
            "title": "Question"
        },
        {
            "location": "/#challenge",
            "text": "What does the dependency graph look like for your Makefile?",
            "title": "Challenge"
        },
        {
            "location": "/#try-it",
            "text": "What happens if you run  make zipf_results.tgz  right now?",
            "title": "Try it"
        },
        {
            "location": "/#practice",
            "text": "Write a recipe for  abyss.png .   Once you've written a recipe for  abyss.png  you should be able to\nrun  make zipf_results.tgz .  Let's delete all of our files and try it out.  rm abyss.* isles.*\nmake zipf_results.tgz  You should get the something like the following output\n(the order may be different)\nto your terminal:  ./wordcount.py books/abyss.txt abyss.dat\n./wordcount.py books/isles.txt isles.dat\n./plotcount.py abyss.dat abyss.png\n./plotcount.py isles.dat isles.png\nrm -rf zipf_results/\nmkdir zipf_results/\ncp isles.dat abyss.dat isles.png abyss.png zipf_results/\ntar -czf zipf_results.tgz zipf_results/\nrm -r zipf_results/  Since you asked for  zipf_results.tgz   Make  looked first for that file.\nNot finding it,  Make  looked for its prerequisites.\nSince none of those existed it remade the ones it could, abyss.dat  and  isles.dat .\nOnce those were finished it was able to make  abyss.png  and isles.png , before finally building  zipf_results.tgz .",
            "title": "Practice"
        },
        {
            "location": "/#try-it_1",
            "text": "What happens if you  touch abyss.dat  and\nthen  make zipf_results.tgz ?   git add Makefile\ngit commit -m \"Finish translating pipeline script to a Makefile.\"\ngit status  Notice all the files that  Git  wants to be tracking?\nLike I said before, we're not going to version control any of the intermediate\nor final products of our pipeline.\nTo reflect this fact add a  .gitignore  file:  *.dat\n*.png\nzipf_results.tgz\nLICENSE.md  git add .gitignore\ngit commit -m \"Have git ignore intermediate data files.\"\ngit status",
            "title": "Try it"
        },
        {
            "location": "/#phony-targets",
            "text": "Sometimes its nice to have targets that don't refer to actual files.  all: isles.png abyss.png zipf_results.tgz  Even though this rule doesn't have a recipe, it does have prerequisites.\nNow, when you run  make all   Make  will do what it needs to to bring\nall three of those targets up to date.  It is traditional for \" all: \" to be the first recipe in a makefile,\nsince the first recipe is what is built by default\nwhen no other target is passed as an argument.  Another traditional target is \" clean \".\nAdd the following to your Makefile.  clean:\n    rm --force *.dat *.png zipf_results.tgz  Running  make clean  will now remove all of the cruft.  Watch out, though!",
            "title": "Phony targets"
        },
        {
            "location": "/#try-it_2",
            "text": "What happens if you create a file named  clean  (i.e.  touch clean )\nand then run  make clean ?   When you run  make clean  you get  make: Nothing to be done for 'clean'. .\nThat's  not  because all those files have already been removed. Make  isn't that smart.\nInstead, make sees that there is already a file named \" clean \" and,\nsince this file is newer than all of its prerequisites (there are none), Make  decides there's nothing left to do.  To avoid this problem add the following to your Makefile.  .PHONY: all clean  This \"special target\" tells  Make  to assume that the targets \"all\", and \"clean\"\nare  not  real files;\nthey're  phony  targets.  git add Makefile\ngit commit -m \"Added 'all' and 'clean' recipes.\"",
            "title": "Try it"
        },
        {
            "location": "/#make-features-45-minutes",
            "text": "Right now our Makefile looks like this:  # Dummy targets\nall: isles.png abyss.png zipf_results.tgz\n\nclean:\n    rm --force *.dat *.png zipf_results.tgz\n\n.PHONY: all clean\n\n# Analysis and plotting\nisles.dat: books/isles.txt\n    ./wordcount.py books/isles.txt isles.dat\n\nisles.png: isles.dat\n    ./plotcount.py isles.dat isles.png\n\nabyss.dat: books/abyss.txt\n    ./wordcount.py books/abyss.txt abyss.dat\n\nabyss.png: abyss.png\n    ./plotcount.py abyss.dat abyss.png\n\n# Archive for sharing\nzipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp isles.dat abyss.dat isles.png abyss.png zipf_results/\n    tar -czf zipf_results.tgz zipf_results/\n    rm -r zipf_results/  Looks good, don't you think?\nNotice the added comments, starting with the \" # \" character just like in\nPython, R, shell, etc.  Using these recipes, a simple call to  make  builds all the same files that\nwe were originally making either manually or using the master script,\nbut with a few bonus features.  Now, if we change one of the inputs, we don't have to rebuild everything.\nInstead,  Make  knows to only rebuild the files that, either directly or\nindirectly, depend on the file that changed.\nThis is called an  incremental build .\nIt's no longer our job to track those dependencies.\nOne fewer cognitive burden getting in the way of research progress!  In addition, a makefile explicitly documents the inputs to and outputs\nfrom every step in the analysis.\nThese are like informal \"USAGE:\" documentation for our scripts.",
            "title": "Make features [45 minutes]"
        },
        {
            "location": "/#parallel-make",
            "text": "And check this out!  make clean\nmake --jobs  Did you see it?\nThe  --jobs  flag (just  -j  works too) tells  Make  to run recipes in  parallel .\nOur dependency graph clearly shows that abyss.dat  and  isles.dat  are mutually independent and can\nboth be built at the same time.\nLikewise for  abyss.png  and  isles.png .\nIf you've got a bunch of independent branches in your analysis, this can\ngreatly speed up your build process.",
            "title": "Parallel Make"
        },
        {
            "location": "/#dry-dont-repeat-yourself",
            "text": "In many programming language, the bulk of the language features are there\nto allow the programmer to describe long-winded computational routines as\nshort, expressive, beautiful code.\nFeatures in Python or R like user-defined variables and functions are\nuseful in part because they mean we don't have to write out (or think about)\nall of the details over and over again.\nThis good habit of writing things out only once is known as the D.R.Y.\nprinciple.  In  Make  a number of features are designed to minimize repetitive code.\nOur current makefile does  not  conform to this principle,\nbut  Make  is perfectly capable of solving the problem.",
            "title": "D.R.Y. (Don't Repeat Yourself)"
        },
        {
            "location": "/#automatic-variables",
            "text": "One overly repetitive part of our Makefile:\nTargets and prerequisites are in both the header  and  the recipe of each rule.  It turns out, that  isles.dat: books/isles.txt\n    ./wordcount.py books/isles.txt isles.dat  can be rewritten as  isles.dat: books/isles.txt\n    ./wordcount.py $^ $@  Here we've replaced the prerequisite \" books/isles.txt \" in the recipe\nwith \" $^ \" and the target \" isles.dat \" with \" $@ \".\nBoth \" $^ \" and \" $@ \" are variables that refer to all of the prerequisites and\ntarget of a rule, respectively.\nIn  Make , variables are referenced with a leading dollar sign symbol.\nWhile we can also define our own variables, Make   automatically  defines a number of variables, including each of these.  zipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp isles.dat abyss.dat isles.png abyss.png zipf_results/\n    tar -czf zipf_results.tgz zipf_results/\n    rm -r zipf_results/  can now be rewritten as  zipf_results.tgz: isles.dat abyss.dat isles.png abyss.png\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp $^ zipf_results/\n    tar -czf $@ zipf_results/\n    rm -r zipf_results/  Phew!  That's much less cluttered,\nand still perfectly understandable once you know what the variables mean.",
            "title": "Automatic variables"
        },
        {
            "location": "/#try-it_3",
            "text": "```bash\nmake clean\nmake isles.dat\n``````````   You should get the same output as last time.\nInternally,  Make  replaced \" $@ \" with \" isles.dat \"\nand \" $^ \" with \" books/isles.txt \"\nbefore running the recipe.",
            "title": "Try it"
        },
        {
            "location": "/#practice_1",
            "text": "Go ahead and rewrite all of the rules in your Makefile to minimize\nrepetition and take advantage of these automatic variables.\nDon't forget to commit your work.",
            "title": "Practice"
        },
        {
            "location": "/#pattern-rules",
            "text": "Another deviation from D.R.Y.:\nWe have nearly identical recipes for  abyss.dat  and  isles.dat .  It turns out we can replace  both  of those rules with just one rule,\nby telling  Make  about the relationships between filename  patterns .  A \"pattern rule\" looks like this:  %.dat: books/%.txt\n    countwords.py $^ $@  Here we've replaced the book name with a percent sign, \" % \".\nThe \" % \" is called the  stem \nand matches any sequence of characters in the target.\n(Kind of like a \" * \" (glob) in a path name, but they are  not  the same.)\nWhatever it matches is then filled in to the prerequisites\nwherever there's a \" % \".  This rule can be interpreted as:   In order to build a file named  [something].dat  (the target)\nfind a file named  books/[that same something].txt  (the prerequisite)\nand run  countwords.py [the prerequisite] [the target] .   Notice how helpful the automatic variables are here.\nThis recipe will work no matter what stem is being matched!  We can replace  both  of the rules that matched this pattern\n( abyss.dat  and  isles.dat ) with just one rule.\nGo ahead and do that in your Makefile.",
            "title": "Pattern rules"
        },
        {
            "location": "/#try-it_4",
            "text": "After you've replaced the two rules with one pattern\nrule, try removing all of the products ( make clean )\nand rerunning the pipeline.  Is anything different now that you're using the pattern rule?  If everything still works, commit your changes to  Git .",
            "title": "Try it"
        },
        {
            "location": "/#practice_2",
            "text": "Replace the recipes for  abyss.png  and  isles.png \nwith a single pattern rule.",
            "title": "Practice"
        },
        {
            "location": "/#challenge_1",
            "text": "Add  books/sierra.txt  to your pipeline.  (i.e.  make all  should plot the word counts and add the plots to zipf_results.tgz )   Commit your changes to  Git  before we move on.",
            "title": "Challenge"
        },
        {
            "location": "/#user-defined-variables",
            "text": "Not all variables in a makefile are of the automatic variety.\nUsers can define their own, as well.  Add this lines at the top of your makefile:  ARCHIVED := isles.dat isles.png \\\n            abyss.dat abyss.png \\\n            sierra.dat sierra.png  Just like many other languages,\nin makefiles \" \\ \" is a line-continuation character.\nThink of this variable definition as a single line without the backslash.  The variable  ARCHIVED  is a list of the files that we want to include in our\ntarball.\nNow wherever we write  ${ARCHIVED}  it will be replaced with that list of files.\nThe dollar sign, \" $ \", and curly-braces, \" {} \", are both mandatory when\ninserting the contents of a variable.  Notice the backslashes in the variable definition\nsplitting the list over three lines, instead of one very long line.\nAlso notice that we assigned to the variable with \" := \".\nThis is generally a Good Idea;\nAssigning with a normal equals sign can result in non-intuitive behavior\n(for reasons we may not talk about).\nFinally, notice that the items in our list are separated by  whitespace ,\nnot commas.\nPrerequisite lists were the same way; this is just how lists of things work in\nmakefiles.\nIf you included commas they would be considered parts of the filenames.  Using this variable we can replace the prerequisites of  zipf_results.tgz .\nThat rule would now be:  zipf_results.tgz: ${ARCHIVED}\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp $^ zipf_results/\n    tar -czf $@ zipf_results/\n    rm -r zipf_results/  We can also use  ${ARCHIVED}  to simplify our cleanup rule.  clean:\n    rm --force ${ARCHIVED} zipf_results.tgz",
            "title": "User defined variables"
        },
        {
            "location": "/#try-it_5",
            "text": "Try running  clean  and then  all .  Does everything still work?",
            "title": "Try it"
        },
        {
            "location": "/#best-practices-for-make-based-projects-60-minutes",
            "text": "A Makefile can be an important part of a reproducible research pipeline.\nHave you noticed how simple it is now to add/remove books from our analysis?\nJust add or remove those files from the definition of  ARCHIVED  or\nthe prerequisites for the  all  target!\nWith the master script  run_pipeline.sh ,\nadding a third book required either more complicated\nor less transparent changes.",
            "title": "Best practices for Make-based projects [60 minutes]"
        },
        {
            "location": "/#whats-a-prerequisite",
            "text": "We've talked a lot about the power of  Make  for\nrebuilding research outputs when input data changes.\nWhen doing novel data analysis, however, it's very common for our  scripts  to\nbe as or  more  dynamic than the data.  What happens when we edit our scripts instead of changing our data?",
            "title": "What's a prerequisite?"
        },
        {
            "location": "/#try-it_6",
            "text": "First, run  make all  so your analysis is up-to-date.  Let's change the default number of entries in the rank/frequency\nplot from 10 to 5.  (Hint: edit the function definition for  plot_word_counts  in plotcounts.py  to read  limit=5 .)  Now run  make all  again.  What happened?   As it stands, we have to run  make clean  followed by  make all \nto update our analysis with the new script.\nWe're missing out on the benefits of incremental analysis when our scripts\nare changing too.  There must be a better way...and there is!  Scripts should be prerequisites too.  Let's edit the pattern rule for  %.png  to include  plotcounts.py \nas a prerequisites.  %.png: plotcounts.py %.dat\n    $^ $@  The header makes sense, but that's a strange looking recipe:\njust two automatic variables.  This recipe works because \" $^ \" is replaced with all of the prerequisites. In order .\nWhen building  abyss.png , for instance, it is replaced with plotcounts.py abyss.dat , which is actually exactly what we want.",
            "title": "Try it"
        },
        {
            "location": "/#try-it_7",
            "text": "What happens when you run the pipeline after modifying your script again?  (Changes to your script can be simulated with  touch plotcounts.py .)",
            "title": "Try it"
        },
        {
            "location": "/#practice_3",
            "text": "Update your other rules to include the relevant scripts as prerequisites.  Commit your changes.",
            "title": "Practice"
        },
        {
            "location": "/#directory-structure",
            "text": "Take a look at all of the clutter in your project directory (run  ls  to\nlist all of the files).\nFor such a small project that's a lot of junk!\nImagine how hard it would be to find your way around this analysis\nif you had more than three steps?\nLet's move some stuff around to make our project easier to navigate.",
            "title": "Directory structure"
        },
        {
            "location": "/#store-scripts-in-scripts",
            "text": "First we'll stow away the scripts.  mkdir scripts/\nmv plotcounts.py wordcount.py scripts/  We also need to update our Makefile to reflect the change:  %.dat: countwords.py books/%.txt\n    $^ $@\n\n%.png: plotcounts.py %.dat\n    $^ $@  becomes:  %.dat: scripts/countwords.py books/%.txt\n    $^ $@\n\n%.png: scripts/plotcounts.py %.dat\n    $^ $@  That's a little more verbose, but it is now explicit\nthat  countwords.py  and  plotcount.py  are scripts.  Git  should have no problem with the move once you tell it which files\nto be aware of.  git add countwords.py plotcounts.py\ngit add scripts/countwords.py scripts/plotcounts.py\ngit add Makefile\ngit commit -m \"Move scripts into a subdirectory.\"  Great!  From here on, when we add new scripts to our analysis they won't\nclutter up our project root.",
            "title": "Store scripts in scripts/"
        },
        {
            "location": "/#hide-intermediate-files-in-data",
            "text": "Speaking of clutter, what are we gonna do about all of these intermediate files!?\nPut 'em in a subdirectory!  mkdir data/\nmv *.tsv data/  And then fix up your Makefile.\nAdjust the relevant lines to look like this.  # ...\n\nARCHIVED := data/isles.dat isles.png \\\n            data/abyss.dat abyss.png \\\n            data/sierra.dat sierra.png\n\n# ...\n\ndata/%.dat: scripts/countwords.py books/%.txt\n    $^ $@\n\n%.png: scripts/plotcounts.py data/%.dat\n\n# ...  Thanks to our  ARCHIVED  variable, making these changes is pretty simple.  We have to make one more change if we don't want  Git  to bother us about\nuntracked files.\nUpdate your  .gitignore .  data/*.dat\n*.png\nzipf_results.tgz\nLICENSE.md  Now commit your changes.  git add Makefile\ngit add .gitignore  Simple!",
            "title": "\"Hide\" intermediate files in data/"
        },
        {
            "location": "/#output-finished-products-to-fig",
            "text": "",
            "title": "Output finished products to fig/"
        },
        {
            "location": "/#practice_4",
            "text": "Move the plots and  zipf_results.tgz  to a directory called  fig/ .   You can call this directory something else if you prefer, but  fig/  seems\nshort and descriptive.",
            "title": "Practice"
        },
        {
            "location": "/#try-it_8",
            "text": "Does your pipeline still execute the way you expect?",
            "title": "Try it"
        },
        {
            "location": "/#file-naming",
            "text": "",
            "title": "File naming"
        },
        {
            "location": "/#use-file-extensions-to-indicate-format",
            "text": "Up to this point, we've been working with three types of data files,\neach with it's own file extension.   ' .txt ' files: the original book in plain-text  ' .dat ' files: word counts and percentages in a plain-text format  ' .png ' files: PNG formatted barplots   Using file extensions like these clearly indicates to anyone not familiar with\nyour project what software to view each file with;\nyou won't get much out of opening a PNG with a text editor.\nWhenever possible, use a widely used extension to make it easy for others\nto understand your data.  File extensions also give us a handle for describing the flow of data in our\npipeline.\nPattern rules rely on this convention.\nOur makefile says that the raw, book data feeds into word count data\nwhich feeds into barplot data.  But the current naming scheme has one obvious ambiguity:\n' .dat ' isn't particularly descriptive.\nLots of file formats can be described as \"data\", including binary formats\nthat would require specialized software to view.\nFor tab-delimited, tabular data (data in rows and columns),\n' .tsv ' is a more precise convention.  Updating our pipeline to use this extension is as simple as find-and-replace\n' .dat ' to ' .tsv ' in our Makefile.\nIf you're tired of  mv -ing your files every time you change your pipeline\nyou can also  make clean  followed by  make all  to check that everything still\nworks.  You might want to update your \" clean \" recipe to remove all the junk\nlike so:  clean:\n    rm -f data/* fig/*  Be sure to commit all of your changes.",
            "title": "Use file extensions to indicate format"
        },
        {
            "location": "/#infix-processing-hints",
            "text": "One of our goals in implementing best practices for our analysis pipeline\nis to make it easy to change it without rewriting everything.\nLet's add a preprocessing step to our analysis that puts\neverything in lowercase before counting words.  The program  tr  (short for \"translate\") is a Unix-style filter that swaps one\nset of characters for another. tr '[:upper:]' '[:lower:]' < [input file] > [output file] \nwill read the mixedcase input file and write all lowercase to\nthe output file.  We can add this to our pipeline by first adding a rule.\nWe know the recipe is going to look like this:  tr '[:upper:]' '[:lower:]' < $^ > $@",
            "title": "Infix processing hints"
        },
        {
            "location": "/#challenge_2",
            "text": "Rewrite your Makefile to update the pipeline with the preprocessing step.   You probably decided to take the pattern  books/%.txt  as the prerequisite,\nbut what did you opt to name the target?  data/%.txt  is an option, but that means we have two files named [bookname].txt , one in  books/  and one in  data/ .\nProbably not the easiest to differentiate.  A better option is to use a more descriptive filename.  data/%.lower.txt: books/%.txt\n    tr '[:upper:]' '[:lower:]' < $^ > $@  By including an  infix  of  .lower.  in our filename it's easy to\nsee that one file is a lowercase version of the mixedcase original.\nNow we can extend our pipeline with a variety of pre- and post-processing\nsteps, give each of them a descriptive infix,\nand the names will be a self-documenting record of its origins.  For reasons which will be explained in a minute, let's also make a dummy\npreprocessing step which will just copy the books verbatim into our data/  directory.  data/%.txt: books/%.txt\n    cp $^ $@  And, in the spirit of infixes, we'll rename  data/%.tsv  to be more descriptive.  data/%.counts.tsv: scripts/wordcount.py data/%.txt\n    $^ $@\n\nfig/%.counts.png: scripts/plotcount.py data/%.counts.tsv\n    $^ $@  Here's the  full  Makefile:  ARCHIVED := data/isles.lower.counts.tsv data/abyss.lower.counts.tsv \\\n            data/sierra.lower.counts.tsv fig/isles.lower.counts.png \\\n            fig/abyss.lower.counts.png fig/sierra.lower.counts.png\n\n# Dummy targets\nall: fig/isles.lower.counts.png fig/abyss.lower.counts.png \\\n        fig/sierra.lower.counts.png zipf_results.tgz\n\nclean:\n    rm --force data/* fig/*\n\n.PHONY: all clean\n\n# Analysis and plotting\ndata/%.txt: books/%.txt\n    cp $^ $@\n\ndata/%.lower.txt: data/%.txt\n    tr '[:upper:]' '[:lower:]' < $^ > $@\n\ndata/%.counts.tsv: scripts/wordcount.py data/%.txt\n    $^ $@\n\nfig/%.counts.png: scripts/plotcount.py data/%.counts.tsv\n    $^ $@\n\n# Archive for sharing\nzipf_results.tgz: ${ARCHIVED}\n    rm -rf zipf_results/\n    mkdir zipf_results/\n    cp $^ zipf_results/\n    tar -czf $@ zipf_results/\n    rm -r zipf_results/  Our filenames are certainly more verbose now, but in exchange we get:   self-documenting filenames  more flexible development  and something else, too...   make clean\nmake fig/abyss.lower.counts.png fig/abyss.counts.png  What happened there?\nWe just built two different barplots, one for our analysis  with  the\npreprocessing step and one  without .\nBoth from the same Makefile.\nBy liberally applying pattern rules and infix filenames\nwe get something like a \"filename language\".\nWe describe the analyses we want to run and then have  Make  figure out the\ndetails.",
            "title": "Challenge"
        },
        {
            "location": "/#practice_5",
            "text": "Update your drawing of the dependency graph.",
            "title": "Practice"
        },
        {
            "location": "/#built-in-testing",
            "text": "It's a Good Idea to check your analysis against some form of ground truth.\nThe simplest version of this is a well-defined dataset that you can\nreason about independent of your code.\nLet's make just such a dataset.\nLet's write a book!  Into a file called  books/test.txt  add something like this:  My Book\nBy Me\n\nThis is a book that I wrote.\n\nThe END  We don't need software to count all of the words in this book, and\nwe can probably imagine exactly what a barplot of the count would look like.\nIf the actual result doesn't look like we expected,\nthen there's probably something wrong with our analysis.\nTesting your scripts with this tiny book is computationally cheap, too.  Let's try it out!  make fig/test.lower.counts.png\nless data/test.lower.counts.tsv  Does your counts data match what you expected?  We should run this test for just about every change we make,\nto our scripts or to our Makefile.\nWe're going to do that a  lot  so we'll make it as easy as possible.  test: fig/test.lower.counts.png\n\n.PHONY: test clean all  You could even add the  test  phony target as the first thing in your Makefile.\nThat way just calling  make  will run your tests.",
            "title": "Built-in Testing"
        },
        {
            "location": "/#practice_6",
            "text": "Add a cleanup target called  testclean  which is specific for\nthe outputs of your test run.   Commit your changes, including  books/test.txt .  git add Makefile\ngit add -f books/test.txt\ngit commit -m \"Add pipeline testing recipe and book.\"",
            "title": "Practice"
        },
        {
            "location": "/#review-version-control",
            "text": "We have been following three guiding principles in our use of version\ncontrol during this lesson.    Use it (always).  Version control is a Good Idea and should be used for any files which\ndescribe your pipeline.\nThis includes notes/documentation/TODOs, scripts, and the Makefiles\nthemselves.    Don't version control raw or processed data which can be recreated.  Raw data stays raw and data cleanup should be part of the pipeline.\nBecause of this, backing up your data is imperative, but version\ncontrol is not usually the best way to do so.\nConsider adding a recipe which downloads raw data using wget  or  curl .  One exception would be test or example data.\nThese should be version controlled, as they are subject to change\nas testing is adapted to the evolving pipeline.  Metadata should also be version controlled, since the format and\ncomposition of the metadata is intimately linked with the analysis pipeline\nitself.    Aim to commit \"atomic\" changes to your pipeline.  This means you should usually run  make test  before committing\nyour changes so that regressions don't need to be fixed\nin subsequent commits.\nCo-dependent updates to metadata, documentation, and testing should\nbe included in the same commit.\nIn a perfect world,  make all  should work, and documentation\nshould be up to date, regardless of what revision has been checked out.\nExcessive application of this principle is ill advised.  A more common problem are behemoth commits which make large numbers of\nunrelated changes.\nIn general, a single sentence commit message should be able to summarize\nall of the changes in a commit.",
            "title": "Review: version control"
        },
        {
            "location": "/#exercises-left-to-the-reader",
            "text": "",
            "title": "Exercises left to the reader"
        },
        {
            "location": "/#challenge_3",
            "text": "Download your data using  Make .",
            "title": "Challenge"
        },
        {
            "location": "/#challenge_4",
            "text": "Bootstrap your local setup using  Make .",
            "title": "Challenge"
        }
    ]
}